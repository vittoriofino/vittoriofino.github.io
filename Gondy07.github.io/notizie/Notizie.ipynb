{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3036ad-5de8-42ed-b1c4-2a2f8ed63df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b127c67-7b8b-437c-9707-7ca565c27ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"./News_Category_Dataset_v3.json\", lines=True) #lines = True si mette perchè ogni riga è un json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43c0296-da34-47bf-aec4-dc7592f0f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U.S. NEWS', 'COMEDY', 'PARENTING', 'WORLD NEWS', 'CULTURE & ARTS',\n",
       "       'TECH', 'SPORTS', 'ENTERTAINMENT', 'POLITICS', 'WEIRD NEWS',\n",
       "       'ENVIRONMENT', 'EDUCATION', 'CRIME', 'SCIENCE', 'WELLNESS',\n",
       "       'BUSINESS', 'STYLE & BEAUTY', 'FOOD & DRINK', 'MEDIA',\n",
       "       'QUEER VOICES', 'HOME & LIVING', 'WOMEN', 'BLACK VOICES', 'TRAVEL',\n",
       "       'MONEY', 'RELIGION', 'LATINO VOICES', 'IMPACT', 'WEDDINGS',\n",
       "       'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE',\n",
       "       'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST',\n",
       "       'FIFTY', 'ARTS', 'DIVORCE'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c1c803-16a6-4ec5-a57c-bb6e350abb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209527 entries, 0 to 209526\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   link               209527 non-null  object        \n",
      " 1   headline           209527 non-null  object        \n",
      " 2   category           209527 non-null  object        \n",
      " 3   short_description  209527 non-null  object        \n",
      " 4   authors            209527 non-null  object        \n",
      " 5   date               209527 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2fcc1d-3d62-45c8-9987-95c0bd112507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.huffpost.com/entry/ap-us-jogger-ab...</td>\n",
       "      <td>Memphis Police: Arrest Made In Jogger's Disapp...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Police in Tennessee say an arrest has been mad...</td>\n",
       "      <td></td>\n",
       "      <td>2022-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>https://www.huffpost.com/entry/trump-org-cfo-t...</td>\n",
       "      <td>Trump Org. CFO To Plead Guilty, Testify Agains...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Allen Weisselberg is charged with taking more ...</td>\n",
       "      <td>Michael R. Sisak, AP</td>\n",
       "      <td>2022-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>https://www.huffpost.com/entry/united-states-m...</td>\n",
       "      <td>Officials: NH Missing Girl Case Shifts To Homi...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Authorities say the search for a New Hampshire...</td>\n",
       "      <td>Holly Ramer, AP</td>\n",
       "      <td>2022-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>https://www.huffpost.com/entry/albuquerque-vol...</td>\n",
       "      <td>Albuquerque Police Share Photo Of Car Eyed In ...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Authorities have said that all four of the kil...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>https://www.huffpost.com/entry/albuquerque-new...</td>\n",
       "      <td>Albuquerque Police Tell Muslim Community To Be...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Police are searching for the shooter, or shoot...</td>\n",
       "      <td>Sara Boboltz</td>\n",
       "      <td>2022-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207483</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/elizabeth...</td>\n",
       "      <td>Elizabeth Smart, Former Kidnapping Victim, Mar...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>ABC News announced in July it had hired Smart ...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207545</th>\n",
       "      <td>https://www.huffingtonpost.comhttp://oldnorthe...</td>\n",
       "      <td>Hannah Kelly, Pastor's Daughter, Dies After Ac...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>20-year-old Hannah Kelley died Saturday mornin...</td>\n",
       "      <td></td>\n",
       "      <td>2012-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208133</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/convict-e...</td>\n",
       "      <td>Tim Cole, Convict Exonerated After Death, Gets...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>The legislature also created the Timothy Cole ...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208134</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/new-york-...</td>\n",
       "      <td>Even When the Subject Is Gun Control, Our Gove...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>I'm an advocate of gun control, and a knee-jer...</td>\n",
       "      <td>Steven Strauss , Contributor\\nJohn L. Weinberg...</td>\n",
       "      <td>2012-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208213</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/karen-swi...</td>\n",
       "      <td>Karen Swift's Funeral Planned For Saturday As ...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Police have not yet released a cause of death ...</td>\n",
       "      <td>David Lohr</td>\n",
       "      <td>2012-02-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3562 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "107     https://www.huffpost.com/entry/ap-us-jogger-ab...   \n",
       "202     https://www.huffpost.com/entry/trump-org-cfo-t...   \n",
       "241     https://www.huffpost.com/entry/united-states-m...   \n",
       "258     https://www.huffpost.com/entry/albuquerque-vol...   \n",
       "269     https://www.huffpost.com/entry/albuquerque-new...   \n",
       "...                                                   ...   \n",
       "207483  https://www.huffingtonpost.com/entry/elizabeth...   \n",
       "207545  https://www.huffingtonpost.comhttp://oldnorthe...   \n",
       "208133  https://www.huffingtonpost.com/entry/convict-e...   \n",
       "208134  https://www.huffingtonpost.com/entry/new-york-...   \n",
       "208213  https://www.huffingtonpost.com/entry/karen-swi...   \n",
       "\n",
       "                                                 headline category  \\\n",
       "107     Memphis Police: Arrest Made In Jogger's Disapp...    CRIME   \n",
       "202     Trump Org. CFO To Plead Guilty, Testify Agains...    CRIME   \n",
       "241     Officials: NH Missing Girl Case Shifts To Homi...    CRIME   \n",
       "258     Albuquerque Police Share Photo Of Car Eyed In ...    CRIME   \n",
       "269     Albuquerque Police Tell Muslim Community To Be...    CRIME   \n",
       "...                                                   ...      ...   \n",
       "207483  Elizabeth Smart, Former Kidnapping Victim, Mar...    CRIME   \n",
       "207545  Hannah Kelly, Pastor's Daughter, Dies After Ac...    CRIME   \n",
       "208133  Tim Cole, Convict Exonerated After Death, Gets...    CRIME   \n",
       "208134  Even When the Subject Is Gun Control, Our Gove...    CRIME   \n",
       "208213  Karen Swift's Funeral Planned For Saturday As ...    CRIME   \n",
       "\n",
       "                                        short_description  \\\n",
       "107     Police in Tennessee say an arrest has been mad...   \n",
       "202     Allen Weisselberg is charged with taking more ...   \n",
       "241     Authorities say the search for a New Hampshire...   \n",
       "258     Authorities have said that all four of the kil...   \n",
       "269     Police are searching for the shooter, or shoot...   \n",
       "...                                                   ...   \n",
       "207483  ABC News announced in July it had hired Smart ...   \n",
       "207545  20-year-old Hannah Kelley died Saturday mornin...   \n",
       "208133  The legislature also created the Timothy Cole ...   \n",
       "208134  I'm an advocate of gun control, and a knee-jer...   \n",
       "208213  Police have not yet released a cause of death ...   \n",
       "\n",
       "                                                  authors       date  \n",
       "107                                                       2022-09-04  \n",
       "202                                  Michael R. Sisak, AP 2022-08-18  \n",
       "241                                       Holly Ramer, AP 2022-08-11  \n",
       "258                                        Nina Golgowski 2022-08-08  \n",
       "269                                          Sara Boboltz 2022-08-06  \n",
       "...                                                   ...        ...  \n",
       "207483                                   Reuters, Reuters 2012-02-19  \n",
       "207545                                                    2012-02-18  \n",
       "208133                                   Reuters, Reuters 2012-02-12  \n",
       "208134  Steven Strauss , Contributor\\nJohn L. Weinberg... 2012-02-12  \n",
       "208213                                         David Lohr 2012-02-11  \n",
       "\n",
       "[3562 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"category\"] == \"CRIME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3a9676-bd1d-4a01-b049-4970a53ed1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "0       https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1       https://www.huffpost.com/entry/american-airlin...   \n",
       "2       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3       https://www.huffpost.com/entry/funniest-parent...   \n",
       "4       https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "...                                                   ...   \n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline   category  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                     authors       date  \n",
       "0       Carla K. Johnson, AP 2022-09-23  \n",
       "1             Mary Papenfuss 2022-09-23  \n",
       "2              Elyse Wanshel 2022-09-23  \n",
       "3           Caroline Bologna 2022-09-23  \n",
       "4             Nina Golgowski 2022-09-22  \n",
       "...                      ...        ...  \n",
       "209522      Reuters, Reuters 2012-01-28  \n",
       "209523                       2012-01-28  \n",
       "209524                       2012-01-28  \n",
       "209525                       2012-01-28  \n",
       "209526                       2012-01-28  \n",
       "\n",
       "[209527 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27db39e-317c-45c4-b54e-05a30999e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La directory di lavoro corrente è: C:\\Users\\Jason\\Desktop\\Data_Science\\Notizie\n",
      "File CSV creato con successo!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Stampa la directory di lavoro corrente\n",
    "current_directory = os.getcwd()\n",
    "print(\"La directory di lavoro corrente è:\", current_directory)\n",
    "\n",
    "# Carica il file JSON\n",
    "data = pd.read_json(\"./News_Category_Dataset_v3.json\", lines=True)\n",
    "\n",
    "# Filtra le notizie per la categoria \"ENVIRONMENT\"\n",
    "environment_news = data[data['category'] == 'ENVIRONMENT']\n",
    "\n",
    "# Salva il risultato in un file CSV\n",
    "environment_news.to_csv(\"environment_news.csv\", index=False)\n",
    "\n",
    "print(\"File CSV creato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c133-65c4-4a26-9ff7-5f1db9190c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def analyze_sentiment(text, api_url=\"http://localhost:1234/api/v0/completions\"):\n",
    "    \"\"\"\n",
    "    Analizza il sentiment di un testo usando un modello LLM locale\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"Classifica il sentiment del testo con un numero:\n",
    "                0 = molto negativo\n",
    "                1 = negativo\n",
    "                2 = positivo\n",
    "                3 = molto positivo\n",
    "                \n",
    "                Testo: {text}\n",
    "                \n",
    "                Numero:\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"granite-3.0-2b-instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_sentiment_score(api_response):\n",
    "    \"\"\"\n",
    "    Estrae il punteggio di sentiment dalla risposta dell'API\n",
    "    \"\"\"\n",
    "    if not api_response or 'choices' not in api_response:\n",
    "        return 1\n",
    "        \n",
    "    try:\n",
    "        response_text = api_response['choices'][0]['text'].strip()\n",
    "        if response_text in ['0', '1', '2', '3']:\n",
    "            return int(response_text)\n",
    "        return 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# Carica il file CSV delle notizie ambientali\n",
    "environment_news = pd.read_csv(\"news_crime.csv\")\n",
    "\n",
    "# Carica i risultati esistenti\n",
    "existing_results = pd.read_csv(\"sentiment_analysis_results.csv\")\n",
    "last_processed_id = 3535  # L'ID dove si è fermata l'analisi\n",
    "\n",
    "# Lista per memorizzare i nuovi risultati\n",
    "results = existing_results.to_dict('records')\n",
    "\n",
    "# Filtra le notizie non ancora analizzate\n",
    "remaining_news = environment_news.iloc[last_processed_id + 1:]\n",
    "total_remaining = len(remaining_news)\n",
    "\n",
    "print(f\"Riprendendo l'analisi da ID {last_processed_id + 1}...\")\n",
    "print(f\"Rimangono {total_remaining} notizie da analizzare...\")\n",
    "\n",
    "# Analizza le notizie rimanenti\n",
    "for index, row in remaining_news.iterrows():\n",
    "    text_to_analyze = f\"{row['headline']}. {row['short_description']}\"\n",
    "    \n",
    "    # Analizza il sentiment\n",
    "    api_response = analyze_sentiment(text_to_analyze)\n",
    "    sentiment_score = extract_sentiment_score(api_response)\n",
    "    \n",
    "    # Stampa progresso e risultato\n",
    "    print(f\"ID: {index} - Sentiment: {sentiment_score}\")\n",
    "    \n",
    "    # Aggiungi risultato\n",
    "    results.append({\n",
    "        \"ID\": index,\n",
    "        \"Date\": row['date'],\n",
    "        \"Sentiment\": sentiment_score\n",
    "    })\n",
    "    \n",
    "    # Ogni 35 notizie, mostra un riepilogo parziale e salva i risultati\n",
    "    if (len(results) - len(existing_results)) % 35 == 0:\n",
    "        print(f\"\\n=== RIEPILOGO PARZIALE ===\")\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        print(\"Distribuzione sentiment:\")\n",
    "        sentiment_counts = temp_df['Sentiment'].value_counts().sort_index()\n",
    "        for sentimen+, count in sentiment_counts.items():\n",
    "            percentage = (count / len(temp_df)) * 100\n",
    "            print(f\"Sentiment {sentiment}: {count} notizie ({percentage:.1f}%)\")\n",
    "        print(\"=====================================\\n\")\n",
    "        \n",
    "        # Salva i risultati parziali\n",
    "        pd.DataFrame(results).to_csv(\"sentiment_analysis_results.csv\", index=False)\n",
    "\n",
    "# Salva i risultati finali\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"sentiment_analysis_results_crime.csv\", index=False)\n",
    "\n",
    "print(\"\\nFile CSV aggiornato con successo!\")\n",
    "\n",
    "# Mostra il riepilogo finale dei risultati\n",
    "print(\"\\n=== RIEPILOGO FINALE ===\")\n",
    "print(\"Distribuzione dei sentiment:\")\n",
    "final_counts = results_df['Sentiment'].value_counts().sort_index()\n",
    "for sentiment, count in final_counts.items():\n",
    "    percentage = (count / len(results_df)) * 100\n",
    "    print(f\"Sentiment {sentiment}: {count} notizie ({percentage:.1f}%)\")\n",
    "print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c42bcd-69c1-4e03-a8df-6143bdbc1a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7818d-b6d7-4210-8495-2f381da5edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "def analyze_sentiment(text, api_url=\"http://localhost:1234/api/v0/completions\"):\n",
    "    \"\"\"\n",
    "    Analizza il sentiment di un testo usando un modello LLM locale\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"Classifica il sentiment del testo con un numero:\n",
    "                 0 = molto negativo\n",
    "                 1 = negativo\n",
    "                 2 = positivo\n",
    "                 3 = molto positivo\n",
    "                 \n",
    "                 Testo: {text}\n",
    "                 \n",
    "                 Numero:\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"granite-3.0-2b-instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except:\n",
    "        return None\n",
    "def extract_sentiment_score(api_response):\n",
    "    \"\"\"\n",
    "    Estrae il punteggio di sentiment dalla risposta dell'API\n",
    "    \"\"\"\n",
    "    if not api_response or 'choices' not in api_response:\n",
    "        return 1\n",
    "        \n",
    "    try:\n",
    "        response_text = api_response['choices'][0]['text'].strip()\n",
    "        if response_text in ['0', '1', '2', '3']:\n",
    "            return int(response_text)\n",
    "        return 1\n",
    "    except:\n",
    "        return 1\n",
    "# Carica il file CSV delle notizie ambientali\n",
    "environment_news = pd.read_csv(\"news_crime.csv\")\n",
    "# Lista per memorizzare i risultati\n",
    "results = []\n",
    "# Contatore per tenere traccia del progresso\n",
    "total_rows = len(environment_news)\n",
    "print(f\"Iniziando l'analisi di {total_rows} notizie...\")\n",
    "# Analizza tutte le notizie\n",
    "for index, row in environment_news.iterrows():\n",
    "    text_to_analyze = f\"{row['headline']}. {row['short_description']}\"\n",
    "    \n",
    "    # Analizza il sentiment\n",
    "    api_response = analyze_sentiment(text_to_analyze)\n",
    "    sentiment_score = extract_sentiment_score(api_response)\n",
    "    \n",
    "    # Stampa progresso e risultato\n",
    "    print(f\"ID: {index} - Sentiment: {sentiment_score}\")\n",
    "    \n",
    "    # Aggiungi risultato\n",
    "    results.append({\n",
    "        \"ID\": index,\n",
    "        \"Date\": row['date'],\n",
    "        \"Sentiment\": sentiment_score\n",
    "    })\n",
    "    \n",
    "    # Ogni 35 notizie, mostra un riepilogo parziale\n",
    "    if (index + 1) % 50 == 0:\n",
    "        print(f\"\\n=== RIEPILOGO DOPO {index + 1} NOTIZIE ===\")\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        print(\"Distribuzione sentiment:\")\n",
    "        sentiment_counts = temp_df['Sentiment'].value_counts().sort_index()\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = (count / len(temp_df)) * 100\n",
    "            print(f\"Sentiment {sentiment}: {count} notizie ({percentage:.1f}%)\")\n",
    "        print(\"=====================================\\n\")\n",
    "# Crea un DataFrame dai risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "# Salva i risultati in un nuovo file CSV\n",
    "results_df.to_csv(\"sentiment_analysis_results.csv\", index=False)\n",
    "print(\"\\nFile CSV creato con successo!\")\n",
    "# Mostra il riepilogo finale dei risultati\n",
    "print(\"\\n=== RIEPILOGO FINALE ===\")\n",
    "print(\"Distribuzione dei sentiment:\")\n",
    "final_counts = results_df['Sentiment'].value_counts().sort_index()\n",
    "for sentiment, count in final_counts.items():\n",
    "    percentage = (count / len(results_df)) * 100\n",
    "    print(f\"Sentiment {sentiment}: {count} notizie ({percentage:.1f}%)\")\n",
    "print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66cabe-ef3c-4762-9ebd-22d5fbb57f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def summarize_text(text, api_url=\"http://localhost:1234/api/v0/completions\", max_attempts=5):\n",
    "    \"\"\"\n",
    "    Summarizes the text using a local LLM model\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"Summarize the following text in exactly ONE word (not 'Crime'), without articles:\n",
    "                Text: {text}\n",
    "                \n",
    "                One-word summary:\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"granite-3.0-2b-instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            response = requests.post(api_url, headers=headers, data=json.dumps(data))\n",
    "            response.raise_for_status()\n",
    "            summary = response.json()['choices'][0]['text'].strip()\n",
    "            if summary.lower() not in [\"the\", \"crime\"] and len(summary.split()) == 1:\n",
    "                return summary\n",
    "            else:\n",
    "                print(f\"Attempt {attempts+1}: {summary}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "        \n",
    "        attempts += 1\n",
    "        time.sleep(1)  # Attende un secondo prima di riprovare\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Carica il file CSV delle notizie ambientali\n",
    "environment_news = pd.read_csv(\"news_crime.csv\")\n",
    "\n",
    "# Risultati iniziali\n",
    "results = []\n",
    "\n",
    "print(\"Analyzing all news articles...\")\n",
    "\n",
    "# Analizza gli articoli di notizie\n",
    "for index, row in environment_news.iterrows():\n",
    "    text_to_analyze = f\"{row['headline']}. {row['short_description']}\"\n",
    "    \n",
    "    # Riassume il testo\n",
    "    summary = summarize_text(text_to_analyze)\n",
    "    \n",
    "    # Stampa progresso e risultato\n",
    "    print(f\"ID: {index} - Summary: {summary}\")\n",
    "    \n",
    "    # Aggiungi risultato, mantenendo il valore di \"Sentiment\" se esistente\n",
    "    result = {\n",
    "        \"ID\": index,\n",
    "        \"Date\": row['date'],\n",
    "        \"Summary\": summary\n",
    "    }\n",
    "    if 'Sentiment' in row:\n",
    "        result[\"Sentiment\"] = row['Sentiment']\n",
    "    results.append(result)\n",
    "    \n",
    "    # Ogni 100 articoli, mostra un riepilogo parziale e salva i risultati\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"\\n=== PARTIAL SUMMARY ===\")\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        print(\"Summary distribution:\")\n",
    "        summary_counts = temp_df['Summary'].value_counts().sort_index()\n",
    "        for summary, count in summary_counts.items():\n",
    "            percentage = (count / len(temp_df)) * 100\n",
    "            print(f\"Summary {summary}: {count} articles ({percentage:.1f}%)\")\n",
    "        print(\"=====================================\\n\")\n",
    "        \n",
    "        # Salva i risultati parziali\n",
    "        temp_df.to_csv(\"sentiment_analysis_results_crime.csv\", index=False)\n",
    "\n",
    "# Salva i risultati finali\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"sentiment_analysis_results_crime.csv\", index=False)\n",
    "\n",
    "print(\"\\nCSV file successfully updated!\")\n",
    "\n",
    "# Mostra il riepilogo finale dei risultati\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(\"Summary distribution:\")\n",
    "final_counts = results_df['Summary'].value_counts().sort_index()\n",
    "for summary, count in final_counts.items():\n",
    "    percentage = (count / len(results_df)) * 100\n",
    "    print(f\"Summary {summary}: {count} articles ({percentage:.1f}%)\")\n",
    "print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a709b0b-6ac5-4e0b-a023-afd55668a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornati 3562 record nel file target.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_sentiment_data(source_file, target_file):\n",
    "    \"\"\"\n",
    "    Copia i valori Sentiment dal file sorgente al file target basandosi sull'ID.\n",
    "    \n",
    "    Parameters:\n",
    "    source_file: Path del file CSV contenente i valori Sentiment da copiare\n",
    "    target_file: Path del file CSV dove aggiungere i valori Sentiment\n",
    "    \"\"\"\n",
    "    # Leggi i file CSV\n",
    "    source_df = pd.read_csv(source_file)\n",
    "    target_df = pd.read_csv(target_file)\n",
    "    \n",
    "    # Crea un dizionario di ID -> Sentiment dal file sorgente\n",
    "    sentiment_dict = dict(zip(source_df['ID'], source_df['Sentiment']))\n",
    "    \n",
    "    # Aggiungi i valori Sentiment al target_df basandosi sull'ID\n",
    "    target_df['Sentiment'] = target_df['ID'].map(sentiment_dict)\n",
    "    \n",
    "    # Salva il risultato sovrascrivendo il file target\n",
    "    target_df.to_csv(target_file, index=False)\n",
    "    \n",
    "    print(f\"Aggiornati {len(target_df)} record nel file target.\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "source_file = 'sentiment_analysis_results_crime_copy.csv'\n",
    "target_file = 'sentiment_analysis_results_crime.csv'\n",
    "\n",
    "merge_sentiment_data(source_file, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b832232d-5662-4a95-8a24-e924ecfd1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word cloud creata e salvata come 'crime_wordcloud.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def create_crime_mask():\n",
    "    # Crea un'immagine vuota\n",
    "    width = 1000\n",
    "    height = 300\n",
    "    mask = Image.new('RGB', (width, height), 'white')\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    \n",
    "    # Usa un font grande e in grassetto\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 200)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Disegna la parola \"CRIME\"\n",
    "    draw.text((50, 50), \"CRIME\", font=font, fill='black')\n",
    "    \n",
    "    # Converti l'immagine in array numpy\n",
    "    mask_array = np.array(mask)\n",
    "    \n",
    "    # Crea la maschera binaria (nero = maschera)\n",
    "    final_mask = mask_array[:, :, 0].astype(np.int_) == 0\n",
    "    \n",
    "    return final_mask\n",
    "\n",
    "def generate_crime_wordcloud(csv_file):\n",
    "    \"\"\"\n",
    "    Genera una word cloud a forma della parola CRIME\n",
    "    \"\"\"\n",
    "    # Leggi il CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Unisci tutti i summary in un unico testo, escludendo \"Unknown\"\n",
    "    text = ' '.join([str(summary) for summary in df['Summary'] if str(summary).lower() != 'unknown'])\n",
    "    \n",
    "    # Crea la maschera\n",
    "    mask = create_crime_mask()\n",
    "    \n",
    "    # Configura il word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=250,\n",
    "        background_color='white',\n",
    "        mask=mask,\n",
    "        contour_width=1,\n",
    "        contour_color='black',\n",
    "        min_font_size=8,\n",
    "        max_font_size=80,\n",
    "        collocations=False\n",
    "    ).generate(text)\n",
    "    \n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Salva l'immagine\n",
    "    plt.savefig('crime_wordcloud.png', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Word cloud creata e salvata come 'crime_wordcloud.png'\")\n",
    "\n",
    "# Esegui il codice\n",
    "csv_file = 'sentiment_analysis_results_crime.csv'\n",
    "generate_crime_wordcloud(csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
